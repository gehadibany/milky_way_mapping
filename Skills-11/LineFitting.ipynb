{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting models to data - straight line fitting\n",
    "\n",
    "Version 0.1 Sep 2019\n",
    "\n",
    "A common data interpretation task is to formulate a mathematical model and fit that model to experimental data. One of the simplest and most used ways of doing this is called _Linear least squares regression_. This is a method of finding the straight line that best fits a set of data points.  It is frequently also referred to as _regression_ , _linear regression_ or _least squares_ .\n",
    "\n",
    "In the least squares method we have a mathematical model, defined with a number of parameters, and some data points. As an example, a straight line can be modelled by the familiar equation $y = mx + c$ which has two parameters: $m$ (the gradient of the line) and $c$ (the intercept). \n",
    "\n",
    "The regression process is one of optimising the fit of the line to the data by establishing the values for the parameters defining the line that minimise the sum of the squared distances between the data points and the straight line model.\n",
    "\n",
    "In this notebook, we'll explore how to do this using some of the fitting tools and functions available with Python.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Imports and housekeeping\n",
    "\n",
    "As usual, one of the first tasks is to import the necessary packages - these include the familiar packages NumPy, Pandas and Matplotlib.  In later cells we will also make use of SciPy for the actual line fitting. \n",
    "\n",
    "Results are persistent from one cell to another, so remember to run cells in sequence.  Most importantly, we need to import packages before using them.  The next cell does this, as well as setting up some other items that we will use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# These two lines enable formatted printing of Pandas DataFrames\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1 Data inspection\n",
    "\n",
    "For the exercises in this notebook, a file with data is provided.  `LineData.csv` is a csv file that can be read in using Pandas in the usual way. \n",
    "\n",
    "This is designed to represent typical experimental data that might be modelled with a straight line. There are 4 columns - `xdat` and `ydat`, which are the $x$ and $y$ values, and `xerr` and `yerr` which are the estimated errors on the $x$ and $y$ data points.\n",
    "\n",
    "As in previous examples, the first task is data inspection, so we will start by reading in the file, inspecting the contents of the resulting DataFrame and making a plot of the raw data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "LineData = pd.read_csv('LineData.csv')\n",
    "\n",
    "# Display first few lines of the dataset\n",
    "LineData.head()\n",
    "\n",
    "# Define variables to refer to the x and y data columns\n",
    "#  - not strictly necessary, but convenient\n",
    "x  = LineData['xdat']\n",
    "y  = LineData['ydat']\n",
    "\n",
    "#Set the size of subsequent plots\n",
    "plt.rcParams['figure.figsize'] = [12, 7.42]\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2 Line fitting using linear regression\n",
    "\n",
    "These data points look as though a straight line will fit.  We can use linear regression to find the line of best fit.\n",
    "\n",
    "This can be done easily using a SciPy function: `scipy.stats.linregress()`, which carries out a linear regression.  As the name implies, this is part of the SciPy library's `stats` package. \n",
    "\n",
    "Linregress() takes two columns of data (representing $x$ and $y$ values) and returns the parameters of the least squares fit.  These are the slope $m$ and intercept $c$ of the fitted straight line, together with some additional parameters:\n",
    "\n",
    "**rvalue**:     The _correlation coefficient_ - a measure of goodness of fit\n",
    "\n",
    "**pvalue**:     A statistical test - not used here\n",
    "   \n",
    "**stderr**:     The standard error (+/-) on the fitted gradient \n",
    "\n",
    "Full documentation of the linregress() function is available on the SciPy website:\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html\n",
    "\n",
    "The program segment in the next cell uses the $x$ and $y$ values from LineData.csv, together with the imported packages NumPy, Pandas and Matplotlib so remember to run the previous cells first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first need to import the linregress() function\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# This single line does the linear regression fit\n",
    "m, c, r, p, stderr = linregress(x, y)\n",
    "\n",
    "print(f'Gradient: {m:.2f} +/- {stderr:.2f}, Intercept: {c:.2f}')\n",
    "print(f'Correlation coefficient: {r:.3f}')\n",
    "\n",
    "# Plot the line on to the data:\n",
    "xstart = x.min() - 0.5                 # calculate start and end x values\n",
    "xend   = x.max() + 0.5                 # for plotting the fitted line \n",
    "xlin   = np.linspace(xstart, xend, 50) # create array of x values for the line\n",
    "ylin   = m*xlin+c                      # generate the y values for the straight line\n",
    "\n",
    "plt.scatter(x, y)                      # plot the data points\n",
    "\n",
    "plt.plot(xlin, ylin, color = 'red')    # plot the fitted line "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Getting more detailed statistical information\n",
    "\n",
    "The statistical information returned by linregress() is sufficient for many purposes, but it is limited to straight line fits, and also returns limited information (for instance, although linregress() calculates the correlation coefficient and the +/- on the gradient, it does not calculate a +/- on the intercept).\n",
    "\n",
    "To fit nonlinear functions, and to obtain more detailed statistical information we can use another SciPy function - `scipy.optimize.curve_fit()`.\n",
    "\n",
    "This is a much more general function than linregress() and allows you to define a model function to be fitted - it is not constrained to a straight line.  In this case, we do actually want a straight line but in other situations it could be some other function such as $y = A x^2$ \n",
    "\n",
    "For use with curve_fit() the model function is defined in a specific manner as a _callable_ function.  This is called by curve_fit() once for each $x$ value. The result is compared each time with the corresponding $y$ value in the supplied data in order to calculate the sum of the squares.  The remaining parameters in the model function (in this case the gradient $m$ and intercept $c$) are provided by curve_fit() and adjusted to minimise the sum of squares.\n",
    "\n",
    "curvefit() returns two objects, one (`popt`) containing the optimised parameters to the model (i.e. the best fit values of $m$ and $c$ in this case) and a _covariance matrix_ (`pcov`) that contains statistical data relating to the fit. \n",
    "\n",
    "The covariance matrix contains two rows of data.  The diagonals of this matrix contain the _squares_ of the errors in $m$ and $c$.  Taking the square roots of these values gives an array, `perr`, containing the +/- on the gradient $m$ and the +/- on the intercept $c$.\n",
    "\n",
    "For more information on the covariance matrix, refer to the curve_fit() documentation:\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html\n",
    "\n",
    "As usual it's probably easiest to present this as an example - using the data from the previous cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# Define a simple, straight line model, accepting an x value or 1D array and using \n",
    "# gradient and intercept parameters supplied by curvefit(). \n",
    "#\n",
    "# If the first parameter (x) is a NumPy array or Pandas series \n",
    "# the function will return a corresponding array of y values.\n",
    "def StrtLineModel(x, m, c):\n",
    "    y_lin = m*x + c\n",
    "    return (y_lin)\n",
    "\n",
    "# Call the fitting function - which returns two objects. The variables 'popt' and 'pcov' \n",
    "# are called this by convention but any names would do.\n",
    "popt, pcov = curve_fit(StrtLineModel, x, y)\n",
    "\n",
    "# if you are interested, un-comment the following  line to print out the\n",
    "# covariance matrix and see how it works\n",
    "# print(pcov)\n",
    "\n",
    "# Calculate the errors on the returned parameters\n",
    "perr = np.sqrt(np.diag(pcov)) # squares of the error values are \n",
    "                              # on the covariance matrix diagonal\n",
    "\n",
    "# For readability, extract values from popt and perr into named variables\n",
    "m_fit = popt[0]\n",
    "c_fit = popt[1]\n",
    "m_err = perr[0]\n",
    "c_err = perr[1]\n",
    "    \n",
    "# Now we can print out the optimised fit parameters and 1-sigma estimates\n",
    "print('fit parameter 1-sigma error')\n",
    "print('***************************************************')\n",
    "print (f'm = {m_fit:.2f} +/- {m_err:.2f}')\n",
    "print (f'c = {c_fit:.2f} +/- {c_err:.2f}')\n",
    "print('***************************************************')\n",
    "\n",
    "# Now plot the data and add the optimised line. \n",
    "# This time we already have a function defined to \n",
    "# calculate the straight line - called this time with the optimised \n",
    "# parameters returned by curvefit()\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, StrtLineModel(x, m_fit, c_fit), color = 'red')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 What if we have estimated errors on the y axis?\n",
    "\n",
    "Experimental data often has (in fact, ought to have) estimates of how accurate the measurement is - each data point has associated errors. \n",
    "\n",
    "Simple functions like linregress() treat all data points as equal. However, it's reasonable to suppose that data points with small errors should be weighted more than those with large estimated errors. Fortunately there are algorithms that do just that.\n",
    "\n",
    "Scipy.optimize.curve_fit() allows us to utilise the error estimates in $y$ that we have in the data. The SciPy method takes the model function you define along with the data points in $x$ and $y$, plus an array containing the $y$ errors. The parameter `absolute_sigma = True` tells the function that we are passing it absolute errors.\n",
    "\n",
    "In the following two cells we will first plot the data with error bars, and then find the optimised fit using curve_fit(), this time taking account of the $y$ errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First obtain y error from the DataFrame we've already read in from LineData.csv\n",
    "yerror = LineData['yerr']\n",
    "\n",
    "# matplotlib makes it easy to plot with error bars\n",
    "plt.errorbar(x, y, yerr = yerror, capsize = 2, fmt = 'o')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell calculates the fit parameters. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once again, define a straight line model function.\n",
    "def StrtLineModel(x, m, c):\n",
    "    y_lin = m*x + c\n",
    "    return (y_lin)\n",
    "\n",
    "# Call the fitting function, which returns two objects. The variables 'popt' and 'pcov' \n",
    "# are called this by convention but any names would do.\n",
    "popt, pcov = curve_fit(StrtLineModel, x, y, sigma = yerror, absolute_sigma = True)\n",
    "\n",
    "# Calculate the accuracy of the returned parameters\n",
    "perr = np.sqrt(np.diag(pcov)) # squares of the error values are \n",
    "                              # on the covariance matrix diagonal\n",
    "    \n",
    "# For readability, extract values from popt and perr into named variables\n",
    "m_fit = popt[0]\n",
    "c_fit = popt[1]\n",
    "m_err = perr[0]\n",
    "c_err = perr[1]\n",
    "\n",
    "#print fit parameters and 1-sigma estimates\n",
    "print('fit parameter 1-sigma error')\n",
    "print('***************************************************')\n",
    "print (f'm = {m_fit:.2f} +- {m_err:.2f}')\n",
    "print (f'c = {c_fit:.2f} +- {c_err:.2f}')\n",
    "print('***************************************************')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Exercise - plot fitted line\n",
    "\n",
    "Having calculated the new fit parameters, create a new plot showing the data points with error bars, together with the fitted line. \n",
    "\n",
    "Enter your solution in the next cell.  Once plotted, compare this with the original fit from Sections 2 and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Confidence bounds or limits\n",
    "\n",
    "Now that we have values in `perr` for the errors on the gradient and the intercept we can use this information to show a set of confidence limits around the best fit line.\n",
    "\n",
    "For this example, the upper and lower limit lines are calculated using the upper and lower values of the intercept, and the upper and lower values of the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare confidence level line limits\n",
    "nstd = 1.0 # to draw 1 sigma interval\n",
    "\n",
    "# Calculate m and c values for upper and lower limit lines\n",
    "# These are NumPy arrays so can carry out array calculations on them\n",
    "# - print them out if you want, to inspect the contents\n",
    "popt_upper = popt + nstd*perr\n",
    "popt_lower = popt - nstd*perr\n",
    "\n",
    "# Calculate y values of the fitted line\n",
    "y_fit = StrtLineModel(x, *popt)\n",
    "\n",
    "# Calculate y values of upper and lower bound lines\n",
    "fit_upper = StrtLineModel(x, *popt_upper)\n",
    "fit_lower = StrtLineModel(x, *popt_lower)\n",
    "\n",
    "# To make a labelled plot, need access to the \n",
    "# figure and axes objects:\n",
    "fig, ax = plt.subplots(1)\n",
    "\n",
    "# Set plot parameters\n",
    "plt.rcParams['xtick.labelsize'] = 18\n",
    "plt.rcParams['ytick.labelsize'] = 18\n",
    "plt.rcParams['font.size'] = 2\n",
    "\n",
    "# axis labels and title\n",
    "plt.xlabel('x', fontsize = 18)\n",
    "plt.ylabel('y', fontsize = 18)\n",
    "plt.title('x vs y with best fit line and 1 sigma confidence', fontsize = 18)\n",
    "\n",
    "# plot the best fit line, data points and error bars\n",
    "plt.plot(x, y_fit, 'r', lw = 2, label = 'Best fit line')\n",
    "plt.plot(x, y, 'o', lw = 2, label = 'Data points')\n",
    "plt.errorbar(x, y, yerr = yerror, capsize = 5, fmt = 'none', ecolor = 'k', label = 'Errors')\n",
    "\n",
    "# Draw shaded area between upper and lower limit lines\n",
    "ax.fill_between(x, fit_upper, fit_lower, alpha = .25, label = '1-sigma interval')\n",
    "\n",
    "plt.legend(loc = 'upper left', fontsize = 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Exercise - plot maximum and minimum gradient lines\n",
    "\n",
    "Instead of plotting upper and lower limit lines, as in the previous cell, create a plot showing the maximum and minimum gradient lines. Enter your solution in the next cell. Once plotted, compare with the plot with upper and lower bounds in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Considering errors in both $x$ and $y$\n",
    "\n",
    "So far we have considered only the errors in the $y$ values. Linear regression tries to minimise the sum of the squares and in curve_fit(), the error is only considered in the $y$ value residual measured to the fitted line. In other words curve_fit() minimises the residuals obtained by measuring a _vertical_ distance from the data point to the model line. \n",
    "\n",
    "This is fine unless we have errors in both $x$ and $y$. In this case it is better to use the _orthogonal_ distance from the data point to the fitted line (i.e. the distance measured at right angles to the line).  This technique is called _Orthogonal Distance Regression (ODR)_. \n",
    "\n",
    "Fortunately, SciPy contains a function, `scipy.odr()` that does this form of regression. \n",
    "\n",
    "To use `scipy.odr()` we need to define the model function in a slightly different way.  The details are in the SciPy documentation:\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/odr.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import odr\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# Again we need to define a modelling function for the straight line\n",
    "# This will be called from scipy.odr() in a similar way to curve_fit()\n",
    "# BUT it is defined in a slightly different way to the function used in curve_fit()\n",
    "# The parameters are contained in a list or tuple, p, and the x values \n",
    "# are supplied at the end, rather than as the first parameter\n",
    "def StrtLineModelOdr(p, x):\n",
    "    m, c = p  \n",
    "    y_lin = m * x  + c\n",
    "    return y_lin\n",
    "\n",
    "# We need both the x and y errors\n",
    "xerror = LineData['xerr']\n",
    "yerror = LineData['yerr']\n",
    "\n",
    "# Build model for fitting. \n",
    "model = odr.Model(StrtLineModelOdr)\n",
    "\n",
    "# Form an odr RealData object\n",
    "rdata = odr.RealData(x, y, sx = xerror, sy = yerror)\n",
    "# Now set up the ODR, \n",
    "# We need an initial guess of m and c - beta0 \n",
    "# We could hard wire this but as our model is a straight line,\n",
    "# we can use scipy.stats.linregress() to get an estimate.\n",
    "# The parameters needed are the slope and intercept (m and c)\n",
    "init_guess = linregress(x, y)[0:2]\n",
    "\n",
    "odr = odr.ODR(rdata, model, beta0 = init_guess )\n",
    "\n",
    "# Run it and get the results\n",
    "result_outputs = odr.run()\n",
    "\n",
    "# Extract the data we need\n",
    "popt = result_outputs.beta\n",
    "\n",
    "# We can get a standard error using the line below (uncomment if you want to)\n",
    "# perr = result_outputs.sd_beta\n",
    "# But we'll use the covariance matrix to compute something closer to a 1 sigma confidence interval\n",
    "perr = np.sqrt(np.diag(result_outputs.cov_beta))\n",
    "\n",
    "# For readability, extract values from popt and perr into named variables\n",
    "m_fit = popt[0]\n",
    "c_fit = popt[1]\n",
    "m_err = perr[0]\n",
    "c_err = perr[1]\n",
    "\n",
    "#print fit parameters and 1-sigma estimates\n",
    "print('fit parameter 1-sigma error')\n",
    "print('***************************************************')\n",
    "print (f'm = {m_fit:.2f} +- {m_err:.2f}')\n",
    "print (f'c = {c_fit:.2f} +- {c_err:.2f}')\n",
    "print('***************************************************')\n",
    "\n",
    "# Now plot the data and fitted line:\n",
    "plt.errorbar(x, y, yerr = yerror, xerr = xerror, fmt = 'o', ecolor = 'k', label = 'Errors')\n",
    "plt.plot(x, StrtLineModelOdr([m_fit, c_fit], x), color = 'red')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Exercise - plot fitted line\n",
    "\n",
    "Expand the example in the previous cell to include a shaded area indicating the error bounds, as in 4.2.\n",
    "\n",
    "Enter your solution in the next cell. Once plotted, compare this with the plot in 4.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
